import numpy as np
import os
import math
import matplotlib.pyplot as plt
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" # Kh·∫Øc ph·ª•c t·∫°m th·ªùi xung ƒë·ªôt OpenMP
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import tensorflow as tf

# Ki·ªÉm tra v√† thi·∫øt l·∫≠p s·ªë l∆∞·ª£ng thread cho CPU
num_cores = os.cpu_count()
print(f"S·ªë l∆∞·ª£ng CPU cores: {num_cores}")
tf.config.threading.set_inter_op_parallelism_threads(num_cores)
tf.config.threading.set_intra_op_parallelism_threads(num_cores)
print("ƒê√£ t·ªëi ∆∞u c·∫•u h√¨nh threading cho CPU")

# K√≠ch th∆∞·ªõc ·∫£nh (nh·ªè h∆°n ƒë·ªÉ ph√π h·ª£p v·ªõi CPU)
IMG_WIDTH, IMG_HEIGHT = 224, 224
IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)
IMG_CHANNELS = 3

# Batch size nh·ªè h∆°n ƒë·ªÉ ph√π h·ª£p v·ªõi CPU
BATCH_SIZE = 16

# Load class indices
class_indices_path = "class_indices.txt"

def load_class_indices(filepath):
    class_indices = {}
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            if ":" not in line or not line.strip():
                continue
            index, label = line.strip().split(":")
            class_indices[int(index)] = label
    return class_indices

class_indices = load_class_indices(class_indices_path)
num_classes = len(class_indices)
print(f"Ph√¢n lo·∫°i {num_classes} l·ªõp: {list(class_indices.values())}")

# ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu
train_dir = "./dogs-vs-cats/train/"
val_dir = "./dogs-vs-cats/validation/"

# Data augmentation n√¢ng cao
train_datagen = ImageDataGenerator(
    rescale=1.0 / 255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    brightness_range=[0.8, 1.2],
    channel_shift_range=30,
)

# Preprocessing cho validation set
val_datagen = ImageDataGenerator(rescale=1.0 / 255)

# T·∫°o generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=True
)

validation_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False  # Kh√¥ng shuffle validation ƒë·ªÉ c√≥ k·∫øt qu·∫£ ƒë√°nh gi√° nh·∫•t qu√°n
)

# S·ªë b∆∞·ªõc m·ªói epoch
steps_per_epoch = math.ceil(train_generator.samples / BATCH_SIZE)
validation_steps = math.ceil(validation_generator.samples / BATCH_SIZE)

print(f"Train samples: {train_generator.samples}, Validation samples: {validation_generator.samples}")
print(f"Steps per epoch: {steps_per_epoch}, Validation steps: {validation_steps}")

# S·ª≠ d·ª•ng MobileNetV2 l√†m backbone - nh·∫π h∆°n v√† hi·ªáu qu·∫£ h∆°n cho CPU
def build_model():
    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)
    )

    # Freeze c√°c layer c∆° s·ªü, ch·ªâ m·ªü kh√≥a 10 layer cu·ªëi ƒë·ªÉ fine-tuning
    for layer in base_model.layers[:-10]:
        layer.trainable = False

    # Th√¥ng b√°o c√°c layer c√≥ th·ªÉ ƒë√†o t·∫°o
    trainable_layers = sum(1 for layer in base_model.layers if layer.trainable)
    total_layers = len(base_model.layers)
    print(f"Fine-tuning {trainable_layers}/{total_layers} layers c·ªßa MobileNetV2")

    # X√¢y d·ª±ng classifier head
    x = base_model.output
    x = GlobalAveragePooling2D()(x)

    # Th√™m dense layers v·ªõi batch normalization
    x = Dense(256, activation=None)(x)
    x = BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)
    x = Dropout(0.4)(x)

    x = Dense(128, activation=None)(x)
    x = BatchNormalization()(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)
    x = Dropout(0.3)(x)

    # Layer ƒë·∫ßu ra
    outputs = Dense(num_classes, activation='softmax')(x)

    # T·∫°o model
    model = Model(inputs=base_model.input, outputs=outputs)

    # Bi√™n d·ªãch v·ªõi Adam optimizer v√† learning rate th·∫•p h∆°n
    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss='categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
    )
    return model

# X√¢y d·ª±ng m√¥ h√¨nh
model = build_model()
model.summary()

# Callbacks
callbacks = [
    # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
    ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        verbose=1,
        save_best_only=True,
        mode='max'
    ),
    # Gi·∫£m learning rate khi kh√¥ng c·∫£i thi·ªán
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=0.00001,
        verbose=1
    ),
    # D·ª´ng s·ªõm ƒë·ªÉ tr√°nh overfitting
    EarlyStopping(
        monitor='val_loss',
        patience=8,
        restore_best_weights=True,
        verbose=1
    )
]

# Train m√¥ h√¨nh
if __name__ == "__main__":
    print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...")

    # ƒê√†o t·∫°o m√¥ h√¨nh
    history = model.fit(
        train_generator,
        epochs=30,
        steps_per_epoch=steps_per_epoch,
        validation_data=validation_generator,
        validation_steps=validation_steps,
        callbacks=callbacks,
        workers=1,  # Gi·ªõi h·∫°n s·ªë worker process
        use_multiprocessing=False,
        verbose=1
    )

    # ƒê√°nh gi√° m√¥ h√¨nh cu·ªëi c√πng
    final_loss, final_accuracy, final_precision, final_recall = model.evaluate(
        validation_generator,
        steps=validation_steps
    )

    print(f"\nK·∫øt qu·∫£ ƒë√°nh gi√° cu·ªëi c√πng:")
    print(f"   ‚îú‚îÄ‚îÄ Accuracy: {final_accuracy:.4f}")
    print(f"   ‚îú‚îÄ‚îÄ Precision: {final_precision:.4f}")
    print(f"   ‚îú‚îÄ‚îÄ Recall: {final_recall:.4f}")
    print(f"   ‚îî‚îÄ‚îÄ F1-Score: {2 * (final_precision * final_recall) / (final_precision + final_recall):.4f}")

    # V·∫Ω bi·ªÉu ƒë·ªì loss v√† accuracy
    plt.figure(figsize=(16, 6))

    # Loss plot
    plt.subplot(1, 3, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Train vs Validation Loss')
    plt.legend()

    # Accuracy plot
    plt.subplot(1, 3, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Train vs Validation Accuracy')
    plt.legend()

    # Learning Rate plot
    plt.subplot(1, 3, 3)
    plt.semilogy(history.history.get('lr', []), label='Learning Rate')
    plt.xlabel('Epochs')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Change')
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_history.png')
    plt.show()

    # L∆∞u m√¥ h√¨nh
    model.save("dog_cat_breed_classifier_mobilenet.h5")
    print("M√¥ h√¨nh ƒë√£ l∆∞u th√†nh c√¥ng: dog_cat_breed_classifier_mobilenet.h5")
    print("Bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán ƒë√£ l∆∞u: training_history.png")

    # T·∫°o m√¥ h√¨nh d·ª± ƒëo√°n nh·∫π h∆°n cho tri·ªÉn khai
    print("üîß T·∫°o m√¥ h√¨nh d·ª± ƒëo√°n t·ªëi ∆∞u...")

    # Convert to TensorFlow Lite
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()

    # L∆∞u m√¥ h√¨nh TFLite
    with open('dog_cat_breed_classifier.tflite', 'wb') as f:
        f.write(tflite_model)

    print("ƒê√£ xu·∫•t m√¥ h√¨nh TFLite: dog_cat_breed_classifier.tflite")
